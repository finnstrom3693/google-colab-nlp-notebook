{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS-yfleN6IK9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zdhtJMbe9ieY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo = \"nqzfaizal77ai/mt-230m-pretrained-en-1bc-exp\""
      ],
      "metadata": {
        "id": "niMEjbQ1h5Ww"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repo = \"Qwen/Qwen3-0.6B\""
      ],
      "metadata": {
        "id": "BqZ4BrW3g6C0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZxdbfhsfbMw_"
      },
      "outputs": [],
      "source": [
        "precision = 'full' # @param [\"half\", \"full\"]\n",
        "if precision == \"half\":\n",
        "    selected_precision = torch.float16\n",
        "elif precision == \"full\":\n",
        "    selected_precision = torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fS7L9e-pbMxF"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czXanMio-f9H"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(repo, torch_dtype=selected_precision)\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h7MJFSlJbMxJ"
      },
      "outputs": [],
      "source": [
        "# @title Hyperparameter Inference\n",
        "max_length = 100  # @param {type:\"integer\"} (maximum length of generated text)\n",
        "min_length = 10   # @param {type:\"integer\"} (minimum length should be less than max_length)\n",
        "top_k = 50        # @param {type:\"integer\"} (typical range 0-100)\n",
        "top_p = 0.92      # @param {type:\"number\"} (should be <= 1.0, typically 0.7-0.95)\n",
        "temperature = 0.7  # @param {type:\"number\"} (typically 0.1-1.0, higher = more random)\n",
        "repetition_penalty = 1.2  # @param {type:\"number\"} (typically 1.0-1.5)\n",
        "do_sample = True   # @param {type:\"boolean\"}\n",
        "use_cache = True   # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The person who likes to incite others has a weakness\""
      ],
      "metadata": {
        "id": "B3xQjarLkRJg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Decoding"
      ],
      "metadata": {
        "id": "cpX_qUzUivjk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1xZcnKx5Txd"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "output = model.generate(\n",
        "                        inputs[\"input_ids\"],\n",
        "                        max_length=max_length,\n",
        "                        min_length=min_length,\n",
        "                        # num_return_sequences=1,\n",
        "                        top_k=top_k,\n",
        "                        top_p=top_p,\n",
        "                        temperature=temperature,\n",
        "                        repetition_penalty=repetition_penalty,\n",
        "                        # num_beams=5,\n",
        "                        # length_penalty=0.8,\n",
        "                        # penalty_alpha=0.5,\n",
        "                        do_sample=True,\n",
        "                        use_cache=True\n",
        "                        )\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy Decoding"
      ],
      "metadata": {
        "id": "M7fUgI29j0aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make greedy decoding code\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "# Greedy Decoding\n",
        "greedy_output = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=max_length,\n",
        "    min_length=min_length,\n",
        "    do_sample=False  # Set do_sample to False for greedy decoding\n",
        ")\n",
        "decoded_output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvoNkdP-iukM",
        "outputId": "d9abe601-acb4-4d42-b693-68b9416b7c4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# Replace newlines with <br>\n",
        "formatted_output = decoded_output.replace(\"\\n\", \"<br>\")\n",
        "\n",
        "# HTML and CSS for displaying the output in a square box\n",
        "html_code = f\"\"\"\n",
        "<div style='border: 2px solid black; padding: 10px; width: 500px; height: 300px; overflow: auto;'>\n",
        "    <p style='font-family: monospace;'>{formatted_output}</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Display the styled output\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "eukuIxnpcTdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}