{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzlrYw0FytsG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bnCPhUIEy49d"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z4Irlw5bzJEF"
      },
      "outputs": [],
      "source": [
        "model_name = \"nqzfaizal77ai/qlgpt-m-en-1bc-exp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnaSzpsG3eIj"
      },
      "outputs": [],
      "source": [
        "model_name = \"nqzfaizal77ai/qlpt-m-en-1bc-exp2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EdCcV198HJyt"
      },
      "outputs": [],
      "source": [
        "model_name = \"EleutherAI/gpt-neo-125m\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OfPc2uWoWKfk"
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/opt-350m\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OT3A3S_BWScd"
      },
      "outputs": [],
      "source": [
        "model_name = \"openai-community/gpt2-medium\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eXy7D5q3WYtC"
      },
      "outputs": [],
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM2-360M\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5nz5CrWszRUL"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyuzlTXwy7pg",
        "outputId": "fb9b6fe8-af9b-4554-88ba-e5a0c2f3e1b0"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6fuCpCRCI7by"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX-i0JLgzXyg"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"mikasenghaas/wikitext-2\", split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoQ5uEit0E3t"
      },
      "source": [
        "Perplexity <br>\n",
        "Note : Lower Better <br>\n",
        " Perplexity is a measure of how well a probability distribution or probability model predicts a sample. In the context of language models, it quantifies how surprised the model is by new data. A lower perplexity score indicates that the model is less \"perplexed\" by the data,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LJNll7qz8BH"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataset, tokenizer, max_length=1024):\n",
        "    device = next(model.parameters()).device\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for example in tqdm(dataset):\n",
        "        input_ids = tokenizer.encode(example['text'], return_tensors='pt').to(device)\n",
        "        if input_ids.size(1) < 2:\n",
        "            continue  # skip very short sequences\n",
        "\n",
        "        # Truncate to max length\n",
        "        for i in range(0, input_ids.size(1), max_length):\n",
        "            input_chunk = input_ids[:, i:i+max_length]\n",
        "            if input_chunk.size(1) < 2:\n",
        "                continue\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_chunk, labels=input_chunk)\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item() * input_chunk.size(1)\n",
        "                total_tokens += input_chunk.size(1)\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N2FK6oO0LD6"
      },
      "outputs": [],
      "source": [
        "perplexity = evaluate(model, dataset, tokenizer)\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGJiNMY66X94"
      },
      "source": [
        "Self-Bleu <br>\n",
        "Note : Lower Better <br>\n",
        "Lower Self-BLEU = Higher Diversity: If the score is low, it means that when one generated sentence is compared to another, their similarity (as measured by BLEU) is low. This indicates that the sentences are distinct, reflecting greater diversity in the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWTzNQFiJg-L"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1IADY5tuJoiM"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GppFfmG6auc",
        "outputId": "0805ca32-3aa8-4329-cd57-b846b8000b66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Self-BLEU: 0.1784 (Lower = More Diverse)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Generate multiple samples from a prompt\n",
        "def generate_samples(prompt, num_samples=50, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_samples\n",
        "    )\n",
        "    return [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
        "\n",
        "# Self-BLEU with BLEU scorer\n",
        "def compute_self_bleu(sentences):\n",
        "    bleu = load(\"bleu\")\n",
        "    scores = []\n",
        "    for i in range(len(sentences)):\n",
        "        references = sentences[:i] + sentences[i+1:]\n",
        "        result = bleu.compute(predictions=[sentences[i]], references=[references])\n",
        "        scores.append(result[\"bleu\"])\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "# Run\n",
        "prompt = \"Hacking drones swarmed the satellite\"\n",
        "samples = generate_samples(prompt)\n",
        "self_bleu_score = compute_self_bleu(samples)\n",
        "print(f\"Self-BLEU: {self_bleu_score:.4f} (Lower = More Diverse)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxIU-ZsC6g-6"
      },
      "source": [
        "Sklearn Topic Consistency <br>\n",
        "Note : Higher is Better <br>\n",
        "What a higher score means: A higher topic_similarity score (closer to 1) indicates that the generated texts (generations) are, on average, more semantically similar to the original prompt. This implies that the model generating these texts has successfully stayed on topic and produced content that aligns with the initial subject matter. <br>\n",
        "What a lower score means: A lower topic_similarity score (closer to 0, or even negative if using different similarity metrics, though cosine similarity for TF-IDF is usually non-negative) indicates that the generated texts are less semantically similar to the prompt. This suggests that the model might have drifted off-topic, generated irrelevant content, or misinterpreted the prompt's intent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVard8A98QU0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJKDHbWd7Qqt",
        "outputId": "372e955c-7e57-4459-fd57-4fb4d7b2d11c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic Similarity (TF-IDF Cosine): 0.1001\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def topic_similarity(prompt, generations):\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    vectors = vectorizer.fit_transform([prompt] + generations)\n",
        "    similarities = cosine_similarity(vectors[0:1], vectors[1:])[0]\n",
        "    return similarities.mean()\n",
        "\n",
        "# Example usage\n",
        "similarity_score = topic_similarity(prompt, samples)\n",
        "print(f\"Topic Similarity (TF-IDF Cosine): {similarity_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_deouQf7SV8"
      },
      "source": [
        "Sentence Tranformers Topic Consistency <br>\n",
        "Note : Higher Better\n",
        "- What a higher score means: A higher `bert_topic_consistency` score (closer to 1.0) signifies that the generated texts (`generations`) are, on average, more semantically aligned and coherent with the original `prompt`. This indicates that the language model producing the generations has successfully understood the prompt's underlying topic and generated relevant content that stays \"on message.\"\n",
        "    - Cosine Similarity Range: Cosine similarity ranges from -1 to 1.\n",
        "        - 1: Perfect similarity (vectors point in the exact same direction).\n",
        "        - 0: No similarity (vectors are orthogonal).\n",
        "        - -1: Perfect dissimilarity (vectors point in opposite directions).\n",
        "    - In the context of sentence embeddings, scores typically range from 0 to 1 for meaningful similarities, as embeddings are often designed to capture positive relatedness.\n",
        "- What a lower score means: A lower `bert_topic_consistency` score (closer to 0) suggests that the generated texts are less semantically related to the `prompt`. This could mean the model drifted off-topic, produced irrelevant information, or fundamentally misunderstood the intent of the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a40FDBBL680Z"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIGw91Y77egi",
        "outputId": "db3b8ba6-8fa5-4a30-c8a5-bdb2b5811424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT Topic Consistency: 0.6798\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def bert_topic_consistency(prompt, generations):\n",
        "    prompt_emb = model.encode(prompt, convert_to_tensor=True)\n",
        "    gen_embs = model.encode(generations, convert_to_tensor=True)\n",
        "    scores = util.cos_sim(prompt_emb, gen_embs)\n",
        "    return scores.mean().item()\n",
        "\n",
        "bert_score = bert_topic_consistency(prompt, samples)\n",
        "print(f\"BERT Topic Consistency: {bert_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MML0hVzm-UcT"
      },
      "source": [
        "Comet Coherence Scoring <br>\n",
        "Note : Higher is better for COMET scores <br>\n",
        "- COMET is a quality estimation metric: COMET is designed to assess the quality of machine-generated text (in this case, the `generations` from your model) against human-written `references`. It predicts how good a translation or text generation is.\n",
        "- Scores indicate similarity and quality: A higher COMET score indicates that the generated text is more similar to the reference text and is considered to be of higher quality. Conversely, a lower score suggests less similarity and lower quality.\n",
        "- Ranges: While the exact range can vary slightly depending on the specific COMET model, scores typically fall within a range where higher values (e.g., closer to 1 or 100, depending on scaling) represent better quality. The `wmt22-cometkiwi-da` model typically outputs scores that can range from negative values up to 1, where higher values are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EpoR400-sTN"
      },
      "outputs": [],
      "source": [
        "!pip install unbabel-comet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_TORZGIjDXI"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9I3txzrjJJ-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"hf_token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a7bda3b65bfe4298ad85d288835fd09a",
            "24a3aa26be79424381649e8ba8af9d1a",
            "5bfc3366032d44e3bc6d99ca4416e790",
            "ca38d8b580b046f0b9180d31c6e10b3a",
            "50cea87e2fe74993a4ea6c76592c3ba4",
            "66d132298bb04edcafea2f90e9ffa927",
            "b762d06c2cc741eb82b048a60f486de9",
            "69db10de41684a28b7654e0f02f75c02",
            "a3dd8db7fd434dcd9d03ee0aef073703",
            "1f3fc5a2af7141a0b23aa55213c8cda2",
            "3002b74b3c1e44ac9b8b36991424cbec"
          ]
        },
        "id": "ap7pgijw-Tzt",
        "outputId": "434a7502-2955-43a5-9881-22f37519af49"
      },
      "outputs": [],
      "source": [
        "from comet import download_model, load_from_checkpoint\n",
        "import torch\n",
        "\n",
        "# Load COMET model\n",
        "try:\n",
        "    comet_model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
        "    comet_model = load_from_checkpoint(comet_model_path)\n",
        "    print(\"COMET model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load COMET model: {e}\")\n",
        "    raise\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def generate_text(prompt, max_length=50, temperature=0.7):\n",
        "    \"\"\"Generate text from the model with more controlled parameters\"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(\n",
        "        **input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=0.9,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Evaluation function using COMET\n",
        "def evaluate_with_comet(prompts, references):\n",
        "    \"\"\"\n",
        "    Evaluate model generations against references using COMET\n",
        "\n",
        "    Args:\n",
        "        prompts: List of prompt strings\n",
        "        references: List of reference strings (one per prompt)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with COMET scores and analysis\n",
        "    \"\"\"\n",
        "    if not isinstance(prompts, list):\n",
        "        prompts = [prompts]\n",
        "    if not isinstance(references, list):\n",
        "        references = [references]\n",
        "\n",
        "    # Generate outputs\n",
        "    generations = [generate_text(prompt) for prompt in prompts]\n",
        "\n",
        "    # Prepare COMET input format\n",
        "    comet_data = [{\"src\": p, \"mt\": g, \"ref\": r}\n",
        "                 for p, g, r in zip(prompts, generations, references)]\n",
        "\n",
        "    # Get COMET scores\n",
        "    try:\n",
        "        comet_output = comet_model.predict(\n",
        "            comet_data,\n",
        "            batch_size=8,\n",
        "            gpus=1 if torch.cuda.is_available() else 0\n",
        "        )\n",
        "\n",
        "        # Extract scores from the output (newer COMET versions return a dict)\n",
        "        if isinstance(comet_output, dict):\n",
        "            comet_scores = comet_output['scores']\n",
        "        else:\n",
        "            comet_scores = comet_output\n",
        "\n",
        "        return {\n",
        "            \"scores\": comet_scores,\n",
        "            \"generations\": generations,\n",
        "            \"average_score\": sum(comet_scores) / len(comet_scores)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in COMET evaluation: {e}\")\n",
        "        return None\n",
        "\n",
        "test_cases = [\n",
        "\n",
        "    # === Politics ===\n",
        "    {\n",
        "        \"prompt\": \"The government passed a new policy regarding\",\n",
        "        \"reference\": \"The government passed a new policy regarding national security to improve border control and intelligence coordination.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Parliament debated a bill on\",\n",
        "        \"reference\": \"Parliament debated a bill on climate change aimed at reducing carbon emissions by 40% over the next decade.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"A constitutional amendment was proposed to\",\n",
        "        \"reference\": \"A constitutional amendment was proposed to lower the voting age to 16 and increase civic engagement among youth.\"\n",
        "    },\n",
        "\n",
        "    # === Military ===\n",
        "    {\n",
        "        \"prompt\": \"The army launched an operation in the\",\n",
        "        \"reference\": \"The army launched an operation in the northern valley to eliminate remaining insurgent strongholds.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Fighter jets conducted airstrikes over\",\n",
        "        \"reference\": \"Fighter jets conducted airstrikes over the mountainous region, targeting weapons caches and militant camps.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Soldiers were deployed to\",\n",
        "        \"reference\": \"Soldiers were deployed to the conflict zone to restore peace and ensure the safety of civilians.\"\n",
        "    },\n",
        "\n",
        "    # === Healthcare ===\n",
        "    {\n",
        "        \"prompt\": \"Doctors discovered a new treatment for\",\n",
        "        \"reference\": \"Doctors discovered a new treatment for Alzheimer's that may significantly slow cognitive decline in early stages.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Hospitals reported a rise in cases of\",\n",
        "        \"reference\": \"Hospitals reported a rise in cases of dengue fever due to increased mosquito activity after the rainy season.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"A new vaccine was developed to fight\",\n",
        "        \"reference\": \"A new vaccine was developed to fight the latest strain of influenza, offering broader protection for high-risk groups.\"\n",
        "    },\n",
        "\n",
        "    # === Islamic ===\n",
        "    {\n",
        "        \"prompt\": \"Muslims around the world celebrated\",\n",
        "        \"reference\": \"Muslims around the world celebrated Eid al-Fitr with prayers, feasts, and acts of charity after a month of fasting.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"The mosque hosted a sermon about\",\n",
        "        \"reference\": \"The mosque hosted a sermon about forgiveness and unity during Friday prayers attended by hundreds.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Ramadan began with\",\n",
        "        \"reference\": \"Ramadan began with the sighting of the crescent moon and the call to the first night prayer.\"\n",
        "    },\n",
        "\n",
        "    # === Economy ===\n",
        "    {\n",
        "        \"prompt\": \"The stock market showed signs of\",\n",
        "        \"reference\": \"The stock market showed signs of recovery after the release of strong quarterly earnings reports.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"The central bank raised interest rates to\",\n",
        "        \"reference\": \"The central bank raised interest rates to combat inflation and stabilize the national currency.\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"Unemployment rates dropped due to\",\n",
        "        \"reference\": \"Unemployment rates dropped due to increased hiring in the technology and service sectors.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "num_test_cases = len(test_cases)\n",
        "categories = {\n",
        "    \"Politics\": range(0, min(3, num_test_cases)),\n",
        "    \"Military\": range(3, min(6, num_test_cases)),\n",
        "    \"Healthcare\": range(6, min(9, num_test_cases)),\n",
        "    \"Islamic\": range(9, min(12, num_test_cases)),\n",
        "    \"Economy\": range(12, min(15, num_test_cases))\n",
        "}\n",
        "\n",
        "# Filter out empty categories\n",
        "categories = {k: v for k, v in categories.items() if len(v) > 0}\n",
        "\n",
        "print(f\"\\nEvaluating {num_test_cases} test cases across {len(categories)} categories\")\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluate_with_comet(\n",
        "    prompts=[tc[\"prompt\"] for tc in test_cases],\n",
        "    references=[tc[\"reference\"] for tc in test_cases]\n",
        ")\n",
        "\n",
        "# Print results\n",
        "if results:\n",
        "    print(\"\\n=== COMET Evaluation Results ===\")\n",
        "    print(f\"Overall Average Score: {results['average_score']:.4f}\")\n",
        "\n",
        "    # Calculate and print category averages if we have categories\n",
        "    if categories:\n",
        "        print(\"\\nCategory Averages:\")\n",
        "        for category_name, indices in categories.items():\n",
        "            try:\n",
        "                category_scores = [results['scores'][i] for i in indices]\n",
        "                avg_score = sum(category_scores) / len(category_scores)\n",
        "                print(f\"{category_name}: {avg_score:.4f}\")\n",
        "            except IndexError:\n",
        "                print(f\"Warning: Couldn't calculate score for {category_name} - index out of range\")\n",
        "\n",
        "    # Print detailed results\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    for i, (prompt, gen, ref, score) in enumerate(zip(\n",
        "        [tc[\"prompt\"] for tc in test_cases],\n",
        "        results[\"generations\"],\n",
        "        [tc[\"reference\"] for tc in test_cases],\n",
        "        results[\"scores\"]\n",
        "    )):\n",
        "        # Find which category this test case belongs to\n",
        "        category = next((cat for cat, indices in categories.items() if i in indices), \"Unknown\")\n",
        "\n",
        "        print(f\"\\nTest Case {i+1} ({category}):\")\n",
        "        print(f\"Prompt:    {prompt}\")\n",
        "        print(f\"Generated: {gen}\")\n",
        "        print(f\"Reference: {ref}\")\n",
        "        print(f\"COMET Score: {score:.4f}\")\n",
        "else:\n",
        "    print(\"Evaluation failed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f3fc5a2af7141a0b23aa55213c8cda2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24a3aa26be79424381649e8ba8af9d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d132298bb04edcafea2f90e9ffa927",
            "placeholder": "​",
            "style": "IPY_MODEL_b762d06c2cc741eb82b048a60f486de9",
            "value": "Fetching 5 files: 100%"
          }
        },
        "3002b74b3c1e44ac9b8b36991424cbec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50cea87e2fe74993a4ea6c76592c3ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bfc3366032d44e3bc6d99ca4416e790": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69db10de41684a28b7654e0f02f75c02",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3dd8db7fd434dcd9d03ee0aef073703",
            "value": 5
          }
        },
        "66d132298bb04edcafea2f90e9ffa927": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69db10de41684a28b7654e0f02f75c02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3dd8db7fd434dcd9d03ee0aef073703": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7bda3b65bfe4298ad85d288835fd09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24a3aa26be79424381649e8ba8af9d1a",
              "IPY_MODEL_5bfc3366032d44e3bc6d99ca4416e790",
              "IPY_MODEL_ca38d8b580b046f0b9180d31c6e10b3a"
            ],
            "layout": "IPY_MODEL_50cea87e2fe74993a4ea6c76592c3ba4"
          }
        },
        "b762d06c2cc741eb82b048a60f486de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca38d8b580b046f0b9180d31c6e10b3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3fc5a2af7141a0b23aa55213c8cda2",
            "placeholder": "​",
            "style": "IPY_MODEL_3002b74b3c1e44ac9b8b36991424cbec",
            "value": " 5/5 [00:00&lt;00:00, 465.34it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
